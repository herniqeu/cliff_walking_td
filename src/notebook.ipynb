{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!pip install plotly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA69LnGO68AL",
        "outputId": "1fe68059-628e-4eb9-ef99-0b96955048c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.io as pio\n",
        "pio.renderers.default = 'colab'\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "uu3F9SL25Wph"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import plotly.graph_objects as goplot_value_function_heatmap\n",
        "import plotly.express as px\n",
        "import plotly.subplots as sp\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Tuple, Any\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "\n",
        "class CliffWalkingEnvironment:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(\"CliffWalking-v0\")\n",
        "        self.nS = self.env.observation_space.n\n",
        "        self.nA = self.env.action_space.n\n",
        "        self.height = 4\n",
        "        self.width = 12\n",
        "\n",
        "    def get_ascii_map(self):\n",
        "        \"\"\"Returns ASCII representation of the environment\"\"\"\n",
        "        grid = [['.' for _ in range(self.width)] for _ in range(self.height)]\n",
        "\n",
        "        # Mark cliff positions\n",
        "        for x in range(1, self.width-1):\n",
        "            grid[self.height-1][x] = 'C'\n",
        "\n",
        "        # Mark start and goal\n",
        "        grid[self.height-1][0] = 'S'\n",
        "        grid[self.height-1][self.width-1] = 'G'\n",
        "\n",
        "        return '\\n'.join([''.join(row) for row in grid])\n",
        "\n",
        "    def state_to_coords(self, state):\n",
        "        \"\"\"Convert state number to grid coordinates\"\"\"\n",
        "        return state // self.width, state % self.width\n",
        "\n",
        "    def coords_to_state(self, row, col):\n",
        "        \"\"\"Convert grid coordinates to state number\"\"\"\n",
        "        return row * self.width + col"
      ],
      "metadata": {
        "id": "QYoaJIQb6O7N"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EpsilonGreedyPolicy:\n",
        "    def __init__(self, env, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
        "        self.env = env\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.q_table = defaultdict(lambda: np.zeros(env.nA))\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"\n",
        "        Epsilon-greedy action selection\n",
        "        \"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.randint(self.env.nA)\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])\n",
        "\n",
        "    def update(self, state, action, reward, next_state, alpha=0.1, gamma=0.99):\n",
        "        \"\"\"\n",
        "        Q-learning update rule\n",
        "        \"\"\"\n",
        "        best_next_action = np.argmax(self.q_table[next_state])\n",
        "        td_target = reward + gamma * self.q_table[next_state][best_next_action]\n",
        "        td_error = td_target - self.q_table[state][action]\n",
        "        self.q_table[state][action] += alpha * td_error\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"\n",
        "        Decay exploration rate\n",
        "        \"\"\"\n",
        "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)"
      ],
      "metadata": {
        "id": "NmHelj8M6TcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxPolicy:\n",
        "    def __init__(self, env, temperature_start=1.0, temperature_end=0.1, temperature_decay=0.995):\n",
        "        self.env = env\n",
        "        self.temperature = temperature_start\n",
        "        self.temperature_end = temperature_end\n",
        "        self.temperature_decay = temperature_decay\n",
        "        self.q_table = defaultdict(lambda: np.zeros(env.nA))\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"\n",
        "        Softmax action selection\n",
        "        \"\"\"\n",
        "        q_values = self.q_table[state]\n",
        "        probabilities = self._softmax(q_values)\n",
        "        return np.random.choice(self.env.nA, p=probabilities)\n",
        "\n",
        "    def _softmax(self, x):\n",
        "        \"\"\"\n",
        "        Compute softmax probabilities\n",
        "        \"\"\"\n",
        "        x = x / self.temperature\n",
        "        exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
        "        return exp_x / exp_x.sum()\n",
        "\n",
        "    def update(self, state, action, reward, next_state, alpha=0.1, gamma=0.99):\n",
        "        \"\"\"\n",
        "        Q-learning update rule\n",
        "        \"\"\"\n",
        "        best_next_action = np.argmax(self.q_table[next_state])\n",
        "        td_target = reward + gamma * self.q_table[next_state][best_next_action]\n",
        "        td_error = td_target - self.q_table[state][action]\n",
        "        self.q_table[state][action] += alpha * td_error\n",
        "\n",
        "    def decay_temperature(self):\n",
        "        \"\"\"\n",
        "        Decay temperature parameter\n",
        "        \"\"\"\n",
        "        self.temperature = max(self.temperature_end, self.temperature * self.temperature_decay)"
      ],
      "metadata": {
        "id": "b0Zvu1RQ6VcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TrainingMetrics:\n",
        "    \"\"\"Store training metrics during learning process\"\"\"\n",
        "    episode_rewards: List[float] = field(default_factory=list)\n",
        "    episode_lengths: List[int] = field(default_factory=list)\n",
        "    state_visits: Dict[int, int] = field(default_factory=lambda: defaultdict(int))\n",
        "    action_history: Dict[Tuple[int, int], int] = field(default_factory=lambda: defaultdict(int))\n",
        "    td_errors: List[float] = field(default_factory=list)\n",
        "    value_history: Dict[int, List[float]] = field(default_factory=lambda: defaultdict(list))\n",
        "    path_sequences: List[List[int]] = field(default_factory=list)\n",
        "    near_cliff_visits: int = 0\n",
        "    recovery_actions: int = 0\n",
        "    success_count: int = 0\n",
        "    q_value_updates: Dict[Tuple[int, int], List[float]] = field(default_factory=lambda: defaultdict(list))\n",
        "    policy_entropy: List[float] = field(default_factory=list)\n",
        "    training_phase: List[str] = field(default_factory=list)"
      ],
      "metadata": {
        "id": "sdVDMSbY6X4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4uwsH5L66aPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TDLearner:\n",
        "    def __init__(self, env, gamma=0.99, alpha=0.1):\n",
        "        \"\"\"\n",
        "        Initialize TD Learning agent\n",
        "\n",
        "        Args:\n",
        "            env: CliffWalkingEnvironment instance\n",
        "            gamma: Discount factor\n",
        "            alpha: Learning rate\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.value_function = defaultdict(float)\n",
        "        self.metrics_collector = MetricsCollector(env)\n",
        "\n",
        "    def learn(self, policy, episodes):\n",
        "        \"\"\"\n",
        "        Enhanced learning loop with metrics collection\n",
        "\n",
        "        Args:\n",
        "            policy: Policy instance (EpsilonGreedy or Softmax)\n",
        "            episodes: Number of training episodes\n",
        "\n",
        "        Returns:\n",
        "            dict: Training results and collected metrics\n",
        "        \"\"\"\n",
        "        history = []\n",
        "        current_path = []\n",
        "\n",
        "        with tqdm(total=episodes, desc=\"Training\", unit=\"episode\") as pbar:\n",
        "            for episode in range(episodes):\n",
        "                state, _ = self.env.env.reset()\n",
        "                done = False\n",
        "                total_reward = 0\n",
        "                steps = 0\n",
        "                current_path = [state]\n",
        "\n",
        "                while not done:\n",
        "                    # Collect pre-action metrics\n",
        "                    self.metrics_collector.update_state_visit(state)\n",
        "\n",
        "                    # Select and execute action\n",
        "                    action = policy.select_action(state)\n",
        "                    self.metrics_collector.update_action_history(state, action)\n",
        "                    next_state, reward, done, truncated, _ = self.env.env.step(action)\n",
        "                    done = done or truncated\n",
        "\n",
        "                    # Update policy and collect metrics\n",
        "                    td_error = self._update_policy(policy, state, action, reward, next_state)\n",
        "                    self.metrics_collector.record_td_error(td_error)\n",
        "\n",
        "                    # Check for near-cliff states and recovery actions\n",
        "                    if self.metrics_collector.check_near_cliff(state):\n",
        "                        self.metrics_collector.metrics.near_cliff_visits += 1\n",
        "                        if reward > -100:  # Successful recovery\n",
        "                            self.metrics_collector.metrics.recovery_actions += 1\n",
        "\n",
        "                    # Update state and collect post-action metrics\n",
        "                    state = next_state\n",
        "                    current_path.append(state)\n",
        "                    total_reward += reward\n",
        "                    steps += 1\n",
        "\n",
        "                # Episode completion metrics\n",
        "                self.metrics_collector.metrics.episode_rewards.append(total_reward)\n",
        "                self.metrics_collector.metrics.episode_lengths.append(steps)\n",
        "                self.metrics_collector.record_path_sequence(current_path)\n",
        "\n",
        "                if reward > -100:  # Successful episode\n",
        "                    self.metrics_collector.metrics.success_count += 1\n",
        "\n",
        "                # Policy parameter decay\n",
        "                if hasattr(policy, 'decay_epsilon'):\n",
        "                    policy.decay_epsilon()\n",
        "                elif hasattr(policy, 'decay_temperature'):\n",
        "                    policy.decay_temperature()\n",
        "\n",
        "                # Record training phase\n",
        "                phase = self._determine_training_phase(episode, episodes)\n",
        "                self.metrics_collector.metrics.training_phase.append(phase)\n",
        "\n",
        "                # Update progress bar\n",
        "                pbar.set_postfix({\n",
        "                    'Reward': f'{total_reward:.2f}',\n",
        "                    'Steps': steps,\n",
        "                    'Success Rate': f'{self.metrics_collector.metrics.success_count/(episode+1):.2%}'\n",
        "                })\n",
        "                pbar.update(1)\n",
        "\n",
        "        return {\n",
        "            'value_function': dict(self.value_function),\n",
        "            'history': history,\n",
        "            'metrics': self.metrics_collector.metrics\n",
        "        }\n",
        "\n",
        "    def _update_policy(self, policy, state, action, reward, next_state) -> float:\n",
        "        \"\"\"\n",
        "        Update policy and calculate TD error\n",
        "\n",
        "        Args:\n",
        "            policy: Current policy\n",
        "            state: Current state\n",
        "            action: Taken action\n",
        "            reward: Received reward\n",
        "            next_state: Resulting state\n",
        "\n",
        "        Returns:\n",
        "            float: TD error\n",
        "        \"\"\"\n",
        "        # Standard Q-learning update\n",
        "        best_next_action = np.argmax(policy.q_table[next_state])\n",
        "        td_target = reward + self.gamma * policy.q_table[next_state][best_next_action]\n",
        "        td_error = td_target - policy.q_table[state][action]\n",
        "        policy.q_table[state][action] += self.alpha * td_error\n",
        "\n",
        "        # Update value function and record metrics\n",
        "        self.value_function[state] = np.max(policy.q_table[state])\n",
        "        self.metrics_collector.update_value_history(state, self.value_function[state])\n",
        "\n",
        "        return td_error\n",
        "\n",
        "    def _determine_training_phase(self, episode: int, total_episodes: int) -> str:\n",
        "        \"\"\"\n",
        "        Determine current training phase\n",
        "\n",
        "        Args:\n",
        "            episode: Current episode number\n",
        "            total_episodes: Total number of episodes\n",
        "\n",
        "        Returns:\n",
        "            str: Current training phase\n",
        "        \"\"\"\n",
        "        if episode < total_episodes * 0.2:\n",
        "            return 'exploration'\n",
        "        elif episode < total_episodes * 0.5:\n",
        "            return 'exploitation_transition'\n",
        "        elif episode < total_episodes * 0.8:\n",
        "            return 'stable_performance'\n",
        "        else:\n",
        "            return 'fine_tuning'"
      ],
      "metadata": {
        "id": "smIo9aya61Vd"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_value_function_heatmap(env, metrics, title=\"Value Function Heatmap\"):\n",
        "    \"\"\"\n",
        "    Create heatmap of final value function\n",
        "\n",
        "    Args:\n",
        "        env: CliffWalkingEnvironment instance\n",
        "        metrics: TrainingMetrics instance\n",
        "        title: Plot title\n",
        "    \"\"\"\n",
        "    values = np.zeros((env.height, env.width))\n",
        "    for state in range(env.nS):\n",
        "        row, col = env.state_to_coords(state)\n",
        "        values[row][col] = metrics.value_history[state][-1] if state in metrics.value_history else 0\n",
        "\n",
        "    fig = go.Figure(data=go.Heatmap(\n",
        "        z=values,\n",
        "        colorscale='Viridis',\n",
        "        text=np.around(values, 2),\n",
        "        texttemplate='%{text}',\n",
        "        textfont={\"size\": 10},\n",
        "        hoverongaps=False\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title=\"Column\",\n",
        "        yaxis_title=\"Row\",\n",
        "        width=800,\n",
        "        height=400\n",
        "    )\n",
        "\n",
        "    fig.show()  # Instead of return fig"
      ],
      "metadata": {
        "id": "hcLVDMtU6vAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_progress(metrics, title=\"Training Progress\"):\n",
        "    \"\"\"\n",
        "    Create multi-metric training progress plot\n",
        "\n",
        "    Args:\n",
        "        metrics: TrainingMetrics instance\n",
        "        title: Plot title\n",
        "    \"\"\"\n",
        "    fig = sp.make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=(\"Episode Rewards\", \"Episode Lengths\",\n",
        "                       \"TD Errors\", \"Success Rate\")\n",
        "    )\n",
        "\n",
        "    # Episode rewards\n",
        "    fig.add_trace(\n",
        "        go.Scatter(y=metrics.episode_rewards, name=\"Reward\"),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    # Episode lengths\n",
        "    fig.add_trace(\n",
        "        go.Scatter(y=metrics.episode_lengths, name=\"Steps\"),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "    # TD Errors\n",
        "    fig.add_trace(\n",
        "        go.Scatter(y=metrics.td_errors, name=\"TD Error\"),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    # Success rate\n",
        "    cumulative_success = np.cumsum(\n",
        "        [1 if r > -100 else 0 for r in metrics.episode_rewards]\n",
        "    ) / np.arange(1, len(metrics.episode_rewards) + 1)\n",
        "    fig.add_trace(\n",
        "        go.Scatter(y=cumulative_success, name=\"Success Rate\"),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        height=800,\n",
        "        width=1000,\n",
        "        showlegend=True\n",
        "    )\n",
        "\n",
        "    fig.show()  # Instead of return fig"
      ],
      "metadata": {
        "id": "-vGFIMyW6zBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = CliffWalkingEnvironment()\n",
        "print(\"\\nEnvironment Map:\")\n",
        "print(env.get_ascii_map())\n",
        "\n",
        "# Initialize policies\n",
        "epsilon_greedy = EpsilonGreedyPolicy(env)\n",
        "softmax = SoftmaxPolicy(env)\n",
        "\n",
        "print(\"\\nStarting Epsilon-Greedy Learning...\")\n",
        "td_learner_eps = TDLearner(env)\n",
        "eps_results = td_learner_eps.learn(epsilon_greedy, episodes=500)\n",
        "\n",
        "print(\"\\nStarting Softmax Learning...\")\n",
        "td_learner_softmax = TDLearner(env)\n",
        "softmax_results = td_learner_softmax.learn(softmax, episodes=500)\n",
        "\n",
        "# Create and display visualizations\n",
        "plot_value_function_heatmap(\n",
        "    env,\n",
        "    eps_results['metrics'],\n",
        "    \"Epsilon-Greedy Value Function\"\n",
        ")\n",
        "\n",
        "plot_value_function_heatmap(\n",
        "    env,\n",
        "    softmax_results['metrics'],\n",
        "    \"Softmax Value Function\"\n",
        ")\n",
        "\n",
        "plot_training_progress(\n",
        "    eps_results['metrics'],\n",
        "    \"Epsilon-Greedy Training Progress\"\n",
        ")\n",
        "\n",
        "plot_training_progress(\n",
        "    softmax_results['metrics'],\n",
        "    \"Softmax Training Progress\"\n",
        ")"
      ],
      "metadata": {
        "id": "HPFWN-Md61Sg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}